# Dagster ë°ì´í„° ê±°ë²„ë„ŒìŠ¤ í”„ë¡œì íŠ¸ ê°€ì´ë“œ

## Dagster ê¸°ì´ˆ

### Dagsterë€?

ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•˜ê³  ê´€ë¦¬í•˜ê¸° ìœ„í•œ ì˜¤í”ˆì†ŒìŠ¤ ë°ì´í„° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ í”Œë«í¼. Pythonìœ¼ë¡œ ì‘ì„±ë˜ë©°, ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì›Œí¬í”Œë¡œìš°ë¥¼ ì•ˆì •ì ì´ê³  ìœ ì§€ë³´ìˆ˜í•˜ê¸° ì‰½ê²Œ ë§Œë“¤ì–´ ì¤Œ

 - **ë°ì´í„° íŒŒì´í”„ë¼ì¸** : ë°ì´í„°ë¥¼ í•œ ê³³ì—ì„œ ë‹¤ë¥¸ ê³³ìœ¼ë¡œ ì´ë™ì‹œí‚¤ê³  ì²˜ë¦¬í•˜ëŠ” ì¼ë ¨ì˜ ê³¼ì • ë˜ëŠ” ì‹œìŠ¤í…œì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì¦‰, ë°ì´í„°ë¥¼ ì›ì²œ ì‹œìŠ¤í…œì—ì„œ ìµœì¢… ëª©ì ì§€(ë°ì´í„° ë ˆì´í¬, ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ ë“±)ë¡œ ì˜®ê¸°ë©´ì„œ í•„ìš”í•œ ê°€ê³µ, ë³€í™˜, ë¶„ì„ ë“±ì˜ ë‹¨ê³„ë¥¼ ê±°ì¹˜ëŠ” ì¼ë ¨ì˜ ê³¼ì •ì„ ìë™í™”í•˜ëŠ” ì‹œìŠ¤í…œ

### ì£¼ìš” íŠ¹ì§•

#### 1. ì½”ë“œ ìš°ì„  ì ‘ê·¼ë²• (Code-First Approach)
- ëª¨ë“  íŒŒì´í”„ë¼ì¸ì´ Python ì½”ë“œë¡œ ì •ì˜
- ë²„ì „ ê´€ë¦¬ ì‹œìŠ¤í…œê³¼ í†µí•©ì´ ìš©ì´

#### 2. íƒ€ì… ì•ˆì „ì„±
- Pythonì˜ íƒ€ì… íŒíŠ¸ë¥¼ í™œìš©í•˜ì—¬ ë°ì´í„° ê²€ì¦ì„ ìˆ˜í–‰
- ëŸ°íƒ€ì„ ì˜¤ë¥˜ë¥¼ ì¤„ì´ê³  ë°ì´í„° í’ˆì§ˆì„ ë³´ì¥

#### 3. DX
- ëŒ€ì‹œë³´ë“œë¥¼ í†µí•´ íŒŒì´í”„ë¼ì¸ ëª¨ë‹ˆí„°ë§ì´ ê°€ëŠ¥
- ì‹¤ì‹œê°„ ë¡œê·¸ì™€ ë©”íŠ¸ë¦­ì„ ì œê³µ
- ë””ë²„ê¹…ê³¼ ë¬¸ì œ í•´ê²°ì— ìš©ì´

### í•µì‹¬ ê°œë…


#### Dagster í•µì‹¬ ê°œë… ê´€ê³„ë„

```mermaid
graph TB
    subgraph "Dagster í•µì‹¬ ê°œë…"
        A[Assets<br/>ë°ì´í„° ìì‚°] --> B[Ops<br/>ì‘ì—… ë‹¨ìœ„]
        B --> C[Jobs<br/>ì›Œí¬í”Œë¡œìš°]
        C --> D[Schedules<br/>ìŠ¤ì¼€ì¤„ë§]
        C --> E[Sensors<br/>ì´ë²¤íŠ¸ íŠ¸ë¦¬ê±°]
        
        F[Resources<br/>ì™¸ë¶€ ì‹œìŠ¤í…œ ì—°ê²°] --> A
        F --> B
        
        G[Config<br/>ì„¤ì •] --> A
        G --> B
        G --> C
    end
    
    subgraph "ì‹¤í–‰ íë¦„"
        H[ë°ì´í„° ì†ŒìŠ¤] --> A
        A --> I[ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤]
        B --> I
        C --> I
    end
    
    subgraph "ëª¨ë‹ˆí„°ë§"
        J[Dagster UI] --> A
        J --> B
        J --> C
        J --> D
        J --> E
    end
    
    style A fill:#ff6b6b,stroke:#333,stroke-width:2px,color:#fff
    style B fill:#4ecdc4,stroke:#333,stroke-width:2px,color:#fff
    style C fill:#45b7d1,stroke:#333,stroke-width:2px,color:#fff
    style D fill:#96ceb4,stroke:#333,stroke-width:2px,color:#fff
    style E fill:#feca57,stroke:#333,stroke-width:2px,color:#fff
    style F fill:#ff9ff3,stroke:#333,stroke-width:2px,color:#fff
    style G fill:#54a0ff,stroke:#333,stroke-width:2px,color:#fff
    style H fill:#5f27cd,stroke:#333,stroke-width:2px,color:#fff
    style I fill:#00d2d3,stroke:#333,stroke-width:2px,color:#fff
    style J fill:#ff9f43,stroke:#333,stroke-width:2px,color:#fff
```

#### Assets
Dagsterì˜ í•µì‹¬ ê°œë…ìœ¼ë¡œ, ë°ì´í„° íŒŒì´í”„ë¼ì¸ì—ì„œ ìƒì„±ë˜ëŠ” ë°ì´í„°ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë‹¨ìœ„
- Assetì€ ì‹¤ì œ ë°ì´í„° ìì²´ë¥¼ ì •ì˜í•˜ëŠ” ê²ƒìœ¼ë¡œ, ë‹¤ìŒê³¼ ê°™ì€ ê²ƒë“¤ì„ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤:
     - ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸”
     - CSV íŒŒì¼
     - JSON íŒŒì¼
     - API ì‘ë‹µ ë°ì´í„°
     - ë³€í™˜ëœ ë°ì´í„°ì…‹

##### ê¸°ë³¸ Assets
```python
from dagster import asset
import pandas as pd

@asset
def raw_data():
    """ì›ì‹œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” asset"""
    return pd.read_csv("data/raw.csv")

@asset
def processed_data(raw_data):
    """ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” asset"""
    return raw_data.dropna()
```

##### Asset Dependency
- ë°ì´í„° íŒŒì´í”„ë¼ì¸ì—ì„œ ì–´ë–¤ assetì´ ë‹¤ë¥¸ assetë¥¼ í•„ìš”ë¡œ í•˜ëŠ” ê´€ê³„ë¥¼ ì •ì˜ í•˜ëŠ” ê²ƒ
    - Dagster ê°€ ì˜ì¡´ì„±ì„ ë¶„ì„í•˜ì—¬ ìˆœì„œë¥¼ ë³´ì¥
    - ì˜ì¡´ì„±ì´ ì—†ëŠ” Asset ë“¤ì€ ë³‘ë ¬ë¡œ ì‹¤í–‰ ê°€ëŠ¥
    - ì¦ë¶„ ì²˜ë¦¬ : ë³€ê²½ëœ Asset ê³¼ ê·¸ ì˜ì¡´ Asset ë“¤ë§Œ ì¬ì‹¤í–‰
    - ì‹œê°í™” : Dagster UI ì—ì„œ ì˜ì¡´ì„± ê·¸ë˜í”„ë¥¼ ì‹œê°ì ìœ¼ë¡œ í™•ì¸ ê°€ëŠ¥

```python
@asset
def customer_orders(customer_data, order_data):
    """ì—¬ëŸ¬ assetì„ ê²°í•©í•˜ëŠ” asset"""
    return customer_data.merge(order_data, on='customer_id')
```

##### Asset Metadata
```python
@asset(
    description="ê³ ê° ì£¼ë¬¸ ë°ì´í„°",
    tags={"team": "analytics", "domain": "sales"},
    freshness_policy=FreshnessPolicy(maximum_lag_minutes=60)
)
def daily_orders():
    return pd.read_csv("data/daily_orders.csv")
```
#### Ops (ì‘ì—…)
OpsëŠ” ë°ì´í„° íŒŒì´í”„ë¼ì¸ì—ì„œ ìˆ˜í–‰í•˜ëŠ” **ê°œë³„ ì‘ì—… ë‹¨ìœ„**ì…ë‹ˆë‹¤. ì‰½ê²Œ ë§í•´ì„œ "ë°ì´í„°ë¥¼ ê°€ê³µí•˜ëŠ” í•¨ìˆ˜"ë¼ê³  ìƒê°í•˜ë©´ ë©ë‹ˆë‹¤.

##### Opsì˜ ì—­í• 
- **ë°ì´í„° ì¶”ì¶œ**: APIì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ê¸°
- **ë°ì´í„° ë³€í™˜**: ë°ì´í„° ì •ì œ, í•„í„°ë§, ì§‘ê³„ ë“±
- **ë°ì´í„° ì ì¬**: ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ê¸°
- **ë°ì´í„° ê²€ì¦**: ë°ì´í„° í’ˆì§ˆ í™•ì¸í•˜ê¸°

##### ê¸°ë³¸ Op ì˜ˆì‹œ
```python
from dagster import op

@op
def extract_customer_data(context):
    """ê³ ê° ë°ì´í„°ë¥¼ APIì—ì„œ ì¶”ì¶œí•˜ëŠ” Op"""
    context.log.info("ê³ ê° ë°ì´í„° ì¶”ì¶œ ì‹œì‘")
    # API í˜¸ì¶œ ë¡œì§
    customers = api.get_customers()
    context.log.info(f"{len(customers)}ëª…ì˜ ê³ ê° ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ")
    return customers

@op
def clean_customer_data(context, customers):
    """ê³ ê° ë°ì´í„°ë¥¼ ì •ì œí•˜ëŠ” Op"""
    context.log.info("ê³ ê° ë°ì´í„° ì •ì œ ì‹œì‘")
    # ì´ë©”ì¼ í˜•ì‹ ê²€ì¦, ì¤‘ë³µ ì œê±° ë“±
    cleaned_customers = remove_duplicates(customers)
    context.log.info(f"{len(cleaned_customers)}ëª…ì˜ ê³ ê° ë°ì´í„° ì •ì œ ì™„ë£Œ")
    return cleaned_customers

@op
def load_customer_data(context, cleaned_customers):
    """ì •ì œëœ ê³ ê° ë°ì´í„°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ëŠ” Op"""
    context.log.info("ê³ ê° ë°ì´í„° ì €ì¥ ì‹œì‘")
    # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ë¡œì§
    database.insert_customers(cleaned_customers)
    context.log.info("ê³ ê° ë°ì´í„° ì €ì¥ ì™„ë£Œ")
```

##### Op Configuration
Opê°€ ì‹¤í–‰ë  ë•Œ í•„ìš”í•œ ì„¤ì •ê°’ë“¤ì„ ì •ì˜í•˜ëŠ” ë°©ë²•

```python
from dagster import op, Config

class CleanOrdersConfig(Config):
    """ì£¼ë¬¸ ë°ì´í„° ì •ì œë¥¼ ìœ„í•œ ì„¤ì •"""
    threshold: int = 100  # ìµœì†Œ ì£¼ë¬¸ ê¸ˆì•¡
    remove_duplicates: bool = True  # ì¤‘ë³µ ì œê±° ì—¬ë¶€
    validate_email: bool = True  # ì´ë©”ì¼ ê²€ì¦ ì—¬ë¶€

@op(
    description="ì£¼ë¬¸ ë°ì´í„° ì •ì œ ë° ê²€ì¦",
    tags={"domain": "sales", "team": "data-engineering"},
    config_schema=CleanOrdersConfig
)
def clean_orders(context, orders_df):
    """ì£¼ë¬¸ ë°ì´í„°ë¥¼ ì •ì œí•˜ëŠ” Op"""
    config = context.op_config
    
    context.log.info(f"ì •ì œ ì‹œì‘: {len(orders_df)}ê°œ ì£¼ë¬¸")
    
    # ì„¤ì •ê°’ì— ë”°ë¥¸ ë°ì´í„° ì •ì œ
    if config.remove_duplicates:
        orders_df = orders_df.drop_duplicates()
        context.log.info("ì¤‘ë³µ ì£¼ë¬¸ ì œê±° ì™„ë£Œ")
    
    if config.validate_email:
        orders_df = orders_df[orders_df['email'].str.contains('@')]
        context.log.info("ì´ë©”ì¼ ê²€ì¦ ì™„ë£Œ")
    
    # ìµœì†Œ ì£¼ë¬¸ ê¸ˆì•¡ í•„í„°ë§
    filtered_orders = orders_df[orders_df["amount"] > config.threshold]
    context.log.info(f"ì •ì œ ì™„ë£Œ: {len(filtered_orders)}ê°œ ì£¼ë¬¸")
    
    return filtered_orders
```

##### Op Dependencies
ì—¬ëŸ¬ Opê°€ ì–´ë–¤ ìˆœì„œë¡œ ì‹¤í–‰ë˜ì–´ì•¼ í•˜ëŠ”ì§€ë¥¼ ì •ì˜

###### ê¸°ë³¸ ì˜ì¡´ì„±
```python
@op
def extract_customer_data(context):
    """ê³ ê° ë°ì´í„° ì¶”ì¶œ"""
    context.log.info("ê³ ê° ë°ì´í„° ì¶”ì¶œ ì‹œì‘")
    customers = pd.read_csv("data/customers.csv")
    context.log.info(f"{len(customers)}ëª… ê³ ê° ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ")
    return customers

@op
def extract_order_data(context):
    """ì£¼ë¬¸ ë°ì´í„° ì¶”ì¶œ"""
    context.log.info("ì£¼ë¬¸ ë°ì´í„° ì¶”ì¶œ ì‹œì‘")
    orders = pd.read_csv("data/orders.csv")
    context.log.info(f"{len(orders)}ê°œ ì£¼ë¬¸ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ")
    return orders

@op
def merge_customer_orders(context, customers, orders):
    """ê³ ê°ê³¼ ì£¼ë¬¸ ë°ì´í„° ë³‘í•©"""
    context.log.info("ê³ ê°-ì£¼ë¬¸ ë°ì´í„° ë³‘í•© ì‹œì‘")
    merged_data = customers.merge(orders, on='customer_id', how='left')
    context.log.info(f"{len(merged_data)}ê°œ ë³‘í•© ë°ì´í„° ìƒì„± ì™„ë£Œ")
    return merged_data

@op
def load_to_warehouse(context, merged_data):
    """ë³‘í•©ëœ ë°ì´í„°ë¥¼ ì›¨ì–´í•˜ìš°ìŠ¤ì— ì €ì¥"""
    context.log.info("ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ ì €ì¥ ì‹œì‘")
    merged_data.to_csv("warehouse/customer_orders.csv", index=False)
    context.log.info("ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ ì €ì¥ ì™„ë£Œ")
```

###### ë³µì¡í•œ ì˜ì¡´ì„± ì˜ˆì‹œ
```python
@op
def extract_data(context):
    """ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ì¶”ì¶œ"""
    context.log.info("ë‹¤ì¤‘ ì†ŒìŠ¤ ë°ì´í„° ì¶”ì¶œ ì‹œì‘")
    
    # ì—¬ëŸ¬ ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ë™ì‹œì— ì¶”ì¶œ
    customers = pd.read_csv("data/customers.csv")
    orders = pd.read_csv("data/orders.csv")
    products = pd.read_csv("data/products.csv")
    
    context.log.info("ë‹¤ì¤‘ ì†ŒìŠ¤ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ")
    return customers, orders, products

@op
def transform_customers(context, customers):
    """ê³ ê° ë°ì´í„° ë³€í™˜"""
    context.log.info("ê³ ê° ë°ì´í„° ë³€í™˜ ì‹œì‘")
    # ê³ ê° ë°ì´í„° ì •ì œ ë¡œì§
    cleaned_customers = customers.dropna(subset=['email'])
    context.log.info("ê³ ê° ë°ì´í„° ë³€í™˜ ì™„ë£Œ")
    return cleaned_customers

@op
def transform_orders(context, orders):
    """ì£¼ë¬¸ ë°ì´í„° ë³€í™˜"""
    context.log.info("ì£¼ë¬¸ ë°ì´í„° ë³€í™˜ ì‹œì‘")
    # ì£¼ë¬¸ ë°ì´í„° ì •ì œ ë¡œì§
    cleaned_orders = orders[orders['amount'] > 0]
    context.log.info("ì£¼ë¬¸ ë°ì´í„° ë³€í™˜ ì™„ë£Œ")
    return cleaned_orders

@op
def transform_products(context, products):
    """ìƒí’ˆ ë°ì´í„° ë³€í™˜"""
    context.log.info("ìƒí’ˆ ë°ì´í„° ë³€í™˜ ì‹œì‘")
    # ìƒí’ˆ ë°ì´í„° ì •ì œ ë¡œì§
    active_products = products[products['status'] == 'active']
    context.log.info("ìƒí’ˆ ë°ì´í„° ë³€í™˜ ì™„ë£Œ")
    return active_products

@op
def create_final_dataset(context, cleaned_customers, cleaned_orders, active_products):
    """ìµœì¢… ë°ì´í„°ì…‹ ìƒì„±"""
    context.log.info("ìµœì¢… ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘")
    
    # ê³ ê°-ì£¼ë¬¸ ë³‘í•©
    customer_orders = cleaned_customers.merge(
        cleaned_orders, on='customer_id', how='left'
    )
    
    # ì£¼ë¬¸-ìƒí’ˆ ë³‘í•©
    final_dataset = customer_orders.merge(
        active_products, on='product_id', how='left'
    )
    
    context.log.info(f"ìµœì¢… ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {len(final_dataset)}ê°œ ë ˆì½”ë“œ")
    return final_dataset
```

###### ì˜ì¡´ì„±ì˜ ì¥ì 
1. **ìë™ ìˆœì„œ ë³´ì¥**: Dagsterê°€ ì˜ì¡´ì„±ì„ ë¶„ì„í•˜ì—¬ ì˜¬ë°”ë¥¸ ìˆœì„œë¡œ ì‹¤í–‰
2. **ë³‘ë ¬ ì²˜ë¦¬**: ì˜ì¡´ì„±ì´ ì—†ëŠ” Opë“¤ì€ ë™ì‹œì— ì‹¤í–‰ ê°€ëŠ¥
3. **ì¬ì‹¤í–‰ ìµœì í™”**: ë³€ê²½ëœ Opì™€ ê·¸ ì˜ì¡´ Opë“¤ë§Œ ì¬ì‹¤í–‰
4. **ì‹œê°í™”**: Dagster UIì—ì„œ ì˜ì¡´ì„± ê·¸ë˜í”„ë¥¼ ì‹œê°ì ìœ¼ë¡œ í™•ì¸ ê°€ëŠ¥

#### Jobs (ì‘ì—…)
JobsëŠ” ì—¬ëŸ¬ Opë¥¼ ì¡°í•©í•˜ì—¬ ì™„ì „í•œ ì›Œí¬í”Œë¡œìš°ë¥¼ ë§Œë“­ë‹ˆë‹¤.

##### ê¸°ë³¸ Job
```python
from dagster import job

@job
def etl_job():
    load_data(transform_data(extract_data()))
```

##### Job êµ¬ì„±
```python
@job(
    description="ì¼ì¼ ETL íŒŒì´í”„ë¼ì¸",
    tags={"team": "data-engineering"},
    config={
        "ops": {
            "extract_data": {"config": {"source": "database"}},
            "transform_data": {"config": {"threshold": 100}}
        }
    }
)
def daily_etl_job():
    load_data(transform_data(extract_data()))
```

#### Resources (ë¦¬ì†ŒìŠ¤)
ResourcesëŠ” **ì™¸ë¶€ ì‹œìŠ¤í…œê³¼ì˜ ì—°ê²°ì„ ê´€ë¦¬í•˜ëŠ” ë„êµ¬**ì…ë‹ˆë‹¤. ì‰½ê²Œ ë§í•´ì„œ "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°", "API í´ë¼ì´ì–¸íŠ¸", "í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ ì—°ê²°" ë“±ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

##### Resource ì˜ˆì‹œ
```python
from dagster import resource, asset

@resource
def database_connection():
    """PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
    return create_postgres_connection()

@resource
def bigcommerce_api():
    """BigCommerce API í´ë¼ì´ì–¸íŠ¸"""
    return BigCommerceClient(
        access_token="your_token",
        store_hash="your_store"
    )

@resource
def slack_notification():
    """Slack ì•Œë¦¼ í´ë¼ì´ì–¸íŠ¸"""
    return SlackClient(token="your_slack_token")

# Assetì—ì„œ Resource ì‚¬ìš©
@asset(required_resource_keys={"database", "bigcommerce_api"})
def customer_data(context):
    """ê³ ê° ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ëŠ” Asset"""
    # Resource ì‚¬ìš©
    api_client = context.resources.bigcommerce_api
    db_connection = context.resources.database
    
    # APIì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    customers = api_client.get_customers()
    
    # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
    db_connection.insert_customers(customers)
    
    return customers

@asset(required_resource_keys={"slack_notification"})
def notify_success(context, customer_data):
    """ì„±ê³µ ì‹œ Slack ì•Œë¦¼"""
    slack = context.resources.slack_notification
    slack.send_message(
        channel="#data-alerts",
        text=f"ê³ ê° ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(customer_data)}ëª…"
    )
```

##### Resourceì˜ ì¥ì 
1. **ì¬ì‚¬ìš©ì„±**: ì—¬ëŸ¬ Assetì—ì„œ ê°™ì€ Resource ì‚¬ìš© ê°€ëŠ¥
2. **ê´€ë¦¬ ìš©ì´ì„±**: ì—°ê²° ì„¤ì •ì„ í•œ ê³³ì—ì„œ ê´€ë¦¬
3. **ë³´ì•ˆ**: ë¯¼ê°í•œ ì •ë³´(API í‚¤, ë¹„ë°€ë²ˆí˜¸)ë¥¼ ì•ˆì „í•˜ê²Œ ê´€ë¦¬
4. **í…ŒìŠ¤íŠ¸**: í…ŒìŠ¤íŠ¸ ì‹œ Mock Resourceë¡œ ì‰½ê²Œ êµì²´ ê°€ëŠ¥

#### Schedules (ìŠ¤ì¼€ì¤„)
ì •ê¸°ì ì¸ ì‘ì—… ì‹¤í–‰ì„ ìœ„í•œ ìŠ¤ì¼€ì¤„ì„ ì •ì˜í•©ë‹ˆë‹¤.

```python
from dagster import schedule, job

@job
def daily_job():
    daily_etl_job()

@schedule(
    job=daily_job,
    cron_schedule="0 2 * * *",  # ë§¤ì¼ ì˜¤ì „ 2ì‹œ
    execution_timezone="Asia/Seoul"
)
def daily_schedule(_context):
    return {}
```

#### Sensors (ì„¼ì„œ)
SensorsëŠ” **ì™¸ë¶€ ì´ë²¤íŠ¸ë¥¼ ê°ì§€í•˜ì—¬ ìë™ìœ¼ë¡œ Jobì„ ì‹¤í–‰**í•˜ëŠ” íŠ¸ë¦¬ê±° ë©”ì»¤ë‹ˆì¦˜ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ìƒˆ íŒŒì¼ì´ ìƒì„±ë˜ê±°ë‚˜, API ì‘ë‹µì´ ë³€ê²½ë˜ê±°ë‚˜, íŠ¹ì • ì¡°ê±´ì´ ë§Œì¡±ë  ë•Œ íŒŒì´í”„ë¼ì¸ì„ ìë™ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.

##### Sensorì˜ ì£¼ìš” íŠ¹ì§•
- **ì´ë²¤íŠ¸ ê¸°ë°˜**: ì™¸ë¶€ ì´ë²¤íŠ¸ì— ë°˜ì‘í•˜ì—¬ ì‹¤í–‰
- **ì‹¤ì‹œê°„ ê°ì§€**: ì£¼ê¸°ì ìœ¼ë¡œ ì¡°ê±´ì„ í™•ì¸
- **ì¤‘ë³µ ë°©ì§€**: ê°™ì€ ì´ë²¤íŠ¸ì— ëŒ€í•´ ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€
- **ì¡°ê±´ë¶€ ì‹¤í–‰**: íŠ¹ì • ì¡°ê±´ì´ ë§Œì¡±ë  ë•Œë§Œ ì‹¤í–‰

##### ê¸°ë³¸ Sensor ì˜ˆì‹œ
```python
from dagster import sensor, job, RunRequest, SensorResult

@job
def process_new_files_job():
    process_new_files()

@sensor(job=process_new_files_job)
def file_sensor(context):
    """ìƒˆ íŒŒì¼ì´ ìƒì„±ë˜ë©´ Jobì„ ì‹¤í–‰í•˜ëŠ” Sensor"""
    new_files = check_for_new_files()
    
    if new_files:
        for file in new_files:
            yield RunRequest(
                run_key=f"file_{file}",  # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ ê³ ìœ  í‚¤
                run_config={
                    "ops": {
                        "process_new_files": {
                            "config": {"file_path": file}
                        }
                    }
                }
            )
        context.log.info(f"ìƒˆ íŒŒì¼ ê°ì§€: {len(new_files)}ê°œ")
```

##### ë‹¤ì–‘í•œ Sensor ì˜ˆì‹œ

###### 1. íŒŒì¼ ì‹œìŠ¤í…œ Sensor
```python
import os
from pathlib import Path

@sensor(job=process_new_files_job)
def csv_file_sensor(context):
    """CSV íŒŒì¼ì´ ìƒì„±ë˜ë©´ Jobì„ ì‹¤í–‰"""
    watch_directory = Path("/data/incoming")
    
    # ê°ì‹œ ë””ë ‰í† ë¦¬ì—ì„œ ìƒˆ CSV íŒŒì¼ í™•ì¸
    csv_files = list(watch_directory.glob("*.csv"))
    
    for csv_file in csv_files:
        # íŒŒì¼ì´ ì™„ì „íˆ ì“°ì—¬ì¡ŒëŠ”ì§€ í™•ì¸ (íŒŒì¼ í¬ê¸°ê°€ 1ë¶„ê°„ ë³€í•˜ì§€ ì•ŠìŒ)
        if is_file_complete(csv_file):
            yield RunRequest(
                run_key=f"csv_{csv_file.name}",
                run_config={
                    "ops": {
                        "process_new_files": {
                            "config": {"file_path": str(csv_file)}
                        }
                    }
                }
            )
            context.log.info(f"ìƒˆ CSV íŒŒì¼ ê°ì§€: {csv_file.name}")

def is_file_complete(file_path):
    """íŒŒì¼ì´ ì™„ì „íˆ ì“°ì—¬ì¡ŒëŠ”ì§€ í™•ì¸"""
    import time
    size1 = file_path.stat().st_size
    time.sleep(1)
    size2 = file_path.stat().st_size
    return size1 == size2
```

###### 2. API ì‘ë‹µ Sensor
```python
import requests
from dagster import sensor, job, RunRequest

@job
def api_data_sync_job():
    sync_api_data()

@sensor(job=api_data_sync_job)
def api_response_sensor(context):
    """API ì‘ë‹µì´ ë³€ê²½ë˜ë©´ Jobì„ ì‹¤í–‰"""
    api_url = "https://api.example.com/data"
    
    try:
        response = requests.get(api_url)
        current_hash = hash(response.content)
        
        # ì´ì „ í•´ì‹œê°’ê³¼ ë¹„êµ
        previous_hash = context.cursor_sensor_value
        
        if previous_hash != current_hash:
            yield RunRequest(
                run_key=f"api_update_{current_hash}",
                run_config={
                    "ops": {
                        "sync_api_data": {
                            "config": {"api_url": api_url}
                        }
                    }
                }
            )
            context.log.info("API ë°ì´í„° ë³€ê²½ ê°ì§€")
        
        # í˜„ì¬ í•´ì‹œê°’ ì €ì¥
        return SensorResult(cursor=str(current_hash))
        
    except Exception as e:
        context.log.error(f"API Sensor ì˜¤ë¥˜: {str(e)}")
```

###### 3. ë°ì´í„°ë² ì´ìŠ¤ ë³€ê²½ Sensor
```python
from dagster import sensor, job, RunRequest
import psycopg2

@job
def sync_database_changes_job():
    sync_database_changes()

@sensor(job=sync_database_changes_job)
def database_change_sensor(context):
    """ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸”ì´ ë³€ê²½ë˜ë©´ Jobì„ ì‹¤í–‰"""
    db_config = {
        "host": "localhost",
        "database": "my_database",
        "user": "my_user",
        "password": "my_password"
    }
    
    try:
        conn = psycopg2.connect(**db_config)
        cursor = conn.cursor()
        
        # í…Œì´ë¸”ì˜ ë§ˆì§€ë§‰ ìˆ˜ì • ì‹œê°„ í™•ì¸
        cursor.execute("""
            SELECT MAX(updated_at) 
            FROM orders 
            WHERE updated_at > %s
        """, (context.cursor_sensor_value or "1970-01-01",))
        
        latest_update = cursor.fetchone()[0]
        
        if latest_update:
            yield RunRequest(
                run_key=f"db_update_{latest_update}",
                run_config={
                    "ops": {
                        "sync_database_changes": {
                            "config": {"since": str(latest_update)}
                        }
                    }
                }
            )
            context.log.info(f"ë°ì´í„°ë² ì´ìŠ¤ ë³€ê²½ ê°ì§€: {latest_update}")
        
        cursor.close()
        conn.close()
        
        return SensorResult(cursor=str(latest_update))
        
    except Exception as e:
        context.log.error(f"ë°ì´í„°ë² ì´ìŠ¤ Sensor ì˜¤ë¥˜: {str(e)}")
```

###### 4. ì¡°ê±´ë¶€ Sensor
```python
from dagster import sensor, job, RunRequest
import datetime

@job
def conditional_job():
    process_conditional_data()

@sensor(job=conditional_job)
def time_based_sensor(context):
    """íŠ¹ì • ì‹œê°„ì—ë§Œ Jobì„ ì‹¤í–‰"""
    now = datetime.datetime.now()
    
    # ì˜¤ì „ 9ì‹œë¶€í„° ì˜¤í›„ 6ì‹œê¹Œì§€ë§Œ ì‹¤í–‰
    if 9 <= now.hour < 18:
        # í‰ì¼ë§Œ ì‹¤í–‰
        if now.weekday() < 5:  # 0=ì›”ìš”ì¼, 4=ê¸ˆìš”ì¼
            yield RunRequest(
                run_key=f"time_based_{now.strftime('%Y%m%d_%H%M')}",
                run_config={
                    "ops": {
                        "process_conditional_data": {
                            "config": {"execution_time": str(now)}
                        }
                    }
                }
            )
            context.log.info(f"ì‹œê°„ ê¸°ë°˜ ì‹¤í–‰: {now}")
```

##### Sensor ì„¤ì • ì˜µì…˜
```python
@sensor(
    job=my_job,
    minimum_interval_seconds=30,  # ìµœì†Œ 30ì´ˆ ê°„ê²©ìœ¼ë¡œ ì‹¤í–‰
    description="íŒŒì¼ ë³€ê²½ ê°ì§€ ì„¼ì„œ",
    tags={"team": "data-engineering", "domain": "file-processing"}
)
def configured_sensor(context):
    # Sensor ë¡œì§
    pass
```

##### Sensorì˜ ì¥ì 
1. **ìë™í™”**: ìˆ˜ë™ ê°œì… ì—†ì´ ìë™ ì‹¤í–‰
2. **ì‹¤ì‹œê°„ì„±**: ì´ë²¤íŠ¸ ë°œìƒ ì¦‰ì‹œ ë°˜ì‘
3. **íš¨ìœ¨ì„±**: í•„ìš”í•œ ë•Œë§Œ ì‹¤í–‰í•˜ì—¬ ë¦¬ì†ŒìŠ¤ ì ˆì•½
4. **í™•ì¥ì„±**: ë‹¤ì–‘í•œ ì´ë²¤íŠ¸ íƒ€ì… ì§€ì›
5. **ëª¨ë‹ˆí„°ë§**: Dagster UIì—ì„œ Sensor ìƒíƒœ í™•ì¸ ê°€ëŠ¥

### DBTì™€ í•¨ê»˜ ì‚¬ìš©í•˜ê¸°

Dagsterì™€ DBTëŠ” ìƒí˜¸ ë³´ì™„ì ì¸ ê´€ê³„ë¡œ, í•¨ê»˜ ì‚¬ìš©í•˜ë©´ ë”ìš± ê°•ë ¥í•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### Dagster + DBT ì•„í‚¤í…ì²˜

```mermaid
graph LR
    A[Dagster<br/>Orchestration] --> B[DBT<br/>Transformation]
    B --> C[Data Warehouse<br/>Snowflake/BigQuery/Redshift]
    
    A1[ìŠ¤ì¼€ì¤„ë§] --> A
    A2[ëª¨ë‹ˆí„°ë§] --> A
    A3[ì•Œë¦¼] --> A
    A4[ë¦¬ì†ŒìŠ¤ ê´€ë¦¬] --> A
    
    B1[SQL ë³€í™˜] --> B
    B2[í…ŒìŠ¤íŠ¸] --> B
    B3[ë¬¸ì„œí™”] --> B
    B4[ë²„ì „ ê´€ë¦¬] --> B
```

#### DBT ì‹¤í–‰ ë°©ì‹

DBTëŠ” **Dagster Asset ë‚´ë¶€ì—ì„œ ì‹¤í–‰**ë©ë‹ˆë‹¤. ì¦‰, Dagsterê°€ DBTë¥¼ í˜¸ì¶œí•˜ëŠ” êµ¬ì¡°ì…ë‹ˆë‹¤.

##### 1. DBT Asset ë°©ì‹ (ê¶Œì¥)
```python
from dagster import dbt_assets, AssetExecutionContext
from dagster_dbt import DbtCliResource

@dbt_assets(
    manifest=transformer_project.manifest_path,
    dagster_dbt_translator=CustomDagsterDbtTranslator(),
)
def transformer_dbt_assets(
    context: AssetExecutionContext, 
    dbt: DbtCliResource, 
    config: DbtConfig
):
    """DBT ëª¨ë¸ì„ ì‹¤í–‰í•˜ëŠ” Asset"""
    dbt_args = ["build"]
    if config.full_refresh:
        dbt_args.append("--full-refresh")
    yield from dbt.cli(dbt_args, context=context).stream()
```

#### ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ

##### ì „ì²´ íŒŒì´í”„ë¼ì¸ êµ¬ì¡°
```python
from dagster import asset, dbt_assets, define_asset_job, schedule

# 1. ì›ì‹œ ë°ì´í„° ìˆ˜ì§‘ Asset
@asset
def src_customer(context, bigcommerce, bigquery):
    """BigCommerceì—ì„œ ê³ ê° ë°ì´í„° ìˆ˜ì§‘"""
    customers = bigcommerce.get_customers()
    bigquery.load_table_from_dataframe(customers, "raw_customers")
    return customers

@asset
def src_order(context, bigcommerce, bigquery):
    """BigCommerceì—ì„œ ì£¼ë¬¸ ë°ì´í„° ìˆ˜ì§‘"""
    orders = bigcommerce.get_orders()
    bigquery.load_table_from_dataframe(orders, "raw_orders")
    return orders

# 2. DBT ë³€í™˜ Asset
@dbt_assets(manifest=transformer_project.manifest_path)
def transformer_dbt_assets(context, dbt):
    """DBTë¥¼ í†µí•œ ë°ì´í„° ë³€í™˜"""
    yield from dbt.cli(["build"], context=context).stream()

# 3. í†µí•© Job ì •ì˜
etl_job = define_asset_job(
    name="etl_pipeline",
    selection=AssetSelection.assets(
        src_customer,
        src_order,
        transformer_dbt_assets,
    ),
)

# 4. ìŠ¤ì¼€ì¤„ ì •ì˜
@schedule(
    job=etl_job,
    cron_schedule="0 2 * * *",  # ë§¤ì¼ ì˜¤ì „ 2ì‹œ
    execution_timezone="Asia/Seoul"
)
def daily_etl_schedule(_context):
    return {}
```

#### ì‹¤í–‰ ìˆœì„œ

1. **ì›ì‹œ ë°ì´í„° ìˆ˜ì§‘**: `src_customer`, `src_order` Asset ì‹¤í–‰
2. **DBT ë³€í™˜**: `transformer_dbt_assets` Asset ì‹¤í–‰ (ë‚´ë¶€ì—ì„œ DBT ì‹¤í–‰)
3. **ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤**: ë³€í™˜ëœ ë°ì´í„°ê°€ ìµœì¢… í…Œì´ë¸”ì— ì €ì¥

#### DBT ë³€í™˜ ê³¼ì • ìƒì„¸ ì„¤ëª…

```python
# 1. DBT Asset ì •ì˜
@dbt_assets(manifest=transformer_project.manifest_path)
def transformer_dbt_assets(context, dbt):
    """DBTë¥¼ í†µí•œ ë°ì´í„° ë³€í™˜"""
    # ì´ Assetì´ ì‹¤í–‰ë˜ë©´ ë‚´ë¶€ì ìœ¼ë¡œ DBT ëª…ë ¹ì–´ê°€ ì‹¤í–‰ë©ë‹ˆë‹¤
    yield from dbt.cli(["build"], context=context).stream()
```

**ì‹¤ì œ ì‹¤í–‰ ê³¼ì •:**

1. **Dagsterê°€ `transformer_dbt_assets` Assetì„ ì‹¤í–‰**
2. **Asset ë‚´ë¶€ì—ì„œ DBT ëª…ë ¹ì–´ ì‹¤í–‰**: `dbt build`
3. **DBTê°€ SQL ëª¨ë¸ë“¤ì„ ì‹¤í–‰**:
   ```sql
   -- dim_customer.sql
   SELECT customer_id, name, email FROM raw_customers
   
   -- fct_order.sql  
   SELECT order_id, customer_id, amount FROM raw_orders
   ```
4. **ê²°ê³¼ê°€ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ì— ì €ì¥**:
   - `dim_customer` í…Œì´ë¸” ìƒì„±
   - `fct_order` í…Œì´ë¸” ìƒì„±

**ì¦‰, Dagster Assetì´ DBTë¥¼ "í˜¸ì¶œ"í•˜ëŠ” êµ¬ì¡°ì…ë‹ˆë‹¤:**

```mermaid
graph TD
    A[Dagster Asset ì‹¤í–‰] --> B[DBT ëª…ë ¹ì–´ ì‹¤í–‰]
    B --> C[SQL ëª¨ë¸ ì‹¤í–‰]
    C --> D[ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ì— ì €ì¥]
    
    E[ì›ì‹œ ë°ì´í„°] --> C
    F[DBT ëª¨ë¸ ì •ì˜] --> C
```

#### ì¥ì 

1. **í†µí•©ëœ ëª¨ë‹ˆí„°ë§**: Dagster UIì—ì„œ ì „ì²´ íŒŒì´í”„ë¼ì¸ ëª¨ë‹ˆí„°ë§
2. **ì˜ì¡´ì„± ê´€ë¦¬**: ì›ì‹œ ë°ì´í„° â†’ DBT ë³€í™˜ ìˆœì„œ ìë™ ë³´ì¥
3. **ì—ëŸ¬ ì²˜ë¦¬**: DBT ì‹¤í–‰ ì‹¤íŒ¨ ì‹œ Dagsterì—ì„œ ì—ëŸ¬ ì²˜ë¦¬
4. **ì•Œë¦¼**: DBT ì‹¤í–‰ ê²°ê³¼ë¥¼ Slack ë“±ìœ¼ë¡œ ì•Œë¦¼
5. **ì¬ì‹¤í–‰**: íŠ¹ì • Assetë§Œ ì„ íƒì ìœ¼ë¡œ ì¬ì‹¤í–‰ ê°€ëŠ¥

#### DBT Op êµ¬í˜„

```python
from dagster import op, Config
import subprocess
import os

class DBTConfig(Config):
    project_dir: str
    profiles_dir: str = "~/.dbt"
    target: str = "dev"

@op
def run_dbt_models(context, config: DBTConfig):
    """DBT ëª¨ë¸ì„ ì‹¤í–‰í•˜ëŠ” Op"""
    context.log.info("Starting DBT model run")
    
    cmd = [
        "dbt", "run",
        "--project-dir", config.project_dir,
        "--profiles-dir", config.profiles_dir,
        "--target", config.target
    ]
    
    result = subprocess.run(cmd, capture_output=True, text=True)
    
    if result.returncode != 0:
        raise Exception(f"DBT run failed: {result.stderr}")
    
    context.log.info("DBT model run completed successfully")
    return result.stdout

@op
def run_dbt_tests(context, config: DBTConfig):
    """DBT í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ëŠ” Op"""
    context.log.info("Starting DBT tests")
    
    cmd = [
        "dbt", "test",
        "--project-dir", config.project_dir,
        "--profiles-dir", config.profiles_dir,
        "--target", config.target
    ]
    
    result = subprocess.run(cmd, capture_output=True, text=True)
    
    if result.returncode != 0:
        context.log.warning(f"DBT tests failed: {result.stderr}")
    else:
        context.log.info("DBT tests passed")
    
    return result.stdout
```

#### í†µí•© Job ì˜ˆì‹œ

```python
from dagster import job, asset

@asset
def raw_data():
    """ì›ì‹œ ë°ì´í„° ìˆ˜ì§‘"""
    return extract_from_source()

@asset
def dbt_models(raw_data):
    """DBTë¥¼ í†µí•œ ë°ì´í„° ë³€í™˜"""
    config = DBTConfig(project_dir="./dbt_project")
    return run_dbt_models(config)

@asset
def dbt_tests(dbt_models):
    """DBT í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    config = DBTConfig(project_dir="./dbt_project")
    return run_dbt_tests(config)

@job
def dbt_pipeline():
    """DBT íŒŒì´í”„ë¼ì¸"""
    dbt_tests(dbt_models(raw_data()))
```

### ì„¤ì¹˜ ë° ì‹œì‘

```bash
pip install dagster dagit
```

#### ì²« ë²ˆì§¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
```bash
dagit -f my_pipeline.py
```

### ì¥ì 

1. **ìœ ì—°ì„±**: ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ì™€ ë„êµ¬ì™€ í†µí•© ê°€ëŠ¥
2. **í™•ì¥ì„±**: ëŒ€ê·œëª¨ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ê°€ëŠ¥
3. **ëª¨ë‹ˆí„°ë§**: ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œì™€ ì•Œë¦¼ ê¸°ëŠ¥
4. **í…ŒìŠ¤íŠ¸**: ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ì™€ í†µí•© í…ŒìŠ¤íŠ¸ ì§€ì›
5. **ì»¤ë®¤ë‹ˆí‹°**: í™œë°œí•œ ì˜¤í”ˆì†ŒìŠ¤ ì»¤ë®¤ë‹ˆí‹°

### ì‚¬ìš© ì‚¬ë¡€

- ETL/ELT íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
- ë¨¸ì‹ ëŸ¬ë‹ ì›Œí¬í”Œë¡œìš° ê´€ë¦¬
- ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ êµ¬ì¶•
- ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬
- ë°°ì¹˜ ì‘ì—… ìŠ¤ì¼€ì¤„ë§

---

## ğŸ¯ í”„ë¡œì íŠ¸ ê°œìš”

ì´ í”„ë¡œì íŠ¸ëŠ” **Wisely**ì˜ ë°ì´í„° ê±°ë²„ë„ŒìŠ¤ë¥¼ ìœ„í•œ Dagster ê¸°ë°˜ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤. BigCommerce, PostgreSQL, BigQuery ë“± ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ë¥¼ í†µí•©í•˜ì—¬ ì¼ê´€ëœ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.

### ì£¼ìš” íŠ¹ì§•
- âœ… **ë‹¤ì¤‘ ë°ì´í„° ì†ŒìŠ¤ í†µí•©**: BigCommerce, PostgreSQL, BigQuery
- âœ… **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**: Dagster UIë¥¼ í†µí•œ íŒŒì´í”„ë¼ì¸ ëª¨ë‹ˆí„°ë§
- âœ… **ë°ì´í„° í’ˆì§ˆ ë³´ì¥**: Asset ì²´í¬ë¥¼ í†µí•œ ë°ì´í„° ê²€ì¦
- âœ… **ìë™í™”ëœ ìŠ¤ì¼€ì¤„ë§**: ì •ê¸°ì ì¸ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
- âœ… **ì—ëŸ¬ ì²˜ë¦¬ ë° ì•Œë¦¼**: Slackì„ í†µí•œ ì‹¤ì‹œê°„ ì•Œë¦¼

---

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
data-governance/
â”œâ”€â”€ orchestrator/          # Dagster ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ì½”ë“œ
â”‚   â”œâ”€â”€ assets/           # ë°ì´í„° ìì‚° ì •ì˜
â”‚   â”‚   â”œâ”€â”€ customer.py   # ê³ ê° ë°ì´í„° ìì‚°
â”‚   â”‚   â”œâ”€â”€ order.py      # ì£¼ë¬¸ ë°ì´í„° ìì‚°
â”‚   â”‚   â”œâ”€â”€ product.py    # ìƒí’ˆ ë°ì´í„° ìì‚°
â”‚   â”‚   â””â”€â”€ dbt.py        # DBT í†µí•© ìì‚°
â”‚   â”œâ”€â”€ resources/        # ì™¸ë¶€ ì‹œìŠ¤í…œ ì—°ê²° ë¦¬ì†ŒìŠ¤
â”‚   â”‚   â”œâ”€â”€ bigcommerce.py # BigCommerce API ë¦¬ì†ŒìŠ¤
â”‚   â”‚   â”œâ”€â”€ postgres.py   # PostgreSQL ë¦¬ì†ŒìŠ¤
â”‚   â”‚   â””â”€â”€ gcs_resource.py # Google Cloud Storage ë¦¬ì†ŒìŠ¤
â”‚   â”œâ”€â”€ jobs/             # ì‘ì—… ì •ì˜
â”‚   â”œâ”€â”€ schedules.py      # ìŠ¤ì¼€ì¤„ ì •ì˜
â”‚   â””â”€â”€ definitions.py    # ë©”ì¸ ì •ì˜ íŒŒì¼
â”œâ”€â”€ transformer/          # DBT ë³€í™˜ í”„ë¡œì íŠ¸
â”‚   â”œâ”€â”€ models/          # DBT ëª¨ë¸
â”‚   â”‚   â”œâ”€â”€ dim/         # ì°¨ì› í…Œì´ë¸”
â”‚   â”‚   â”œâ”€â”€ fct/         # íŒ©íŠ¸ í…Œì´ë¸”
â”‚   â”‚   â””â”€â”€ mart/        # ë§ˆíŠ¸ í…Œì´ë¸”
â”‚   â””â”€â”€ dbt_project.yml  # DBT í”„ë¡œì íŠ¸ ì„¤ì •
â””â”€â”€ requirements.txt      # Python ì˜ì¡´ì„±
```

---

## ğŸ”§ í•µì‹¬ êµ¬ì„± ìš”ì†Œ

### 1. Assets (ë°ì´í„° ìì‚°)

#### ê³ ê° ë°ì´í„° ìì‚°
```python
@asset(group_name="customer")
def src_customer(
    context: AssetExecutionContext,
    config: CustomerConfig,
    bigcommerce: BigCommerceResource,
    bigquery: BigQueryResource,
) -> None:
    """BigCommerceì—ì„œ ê³ ê° ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ BigQueryì— ì ì¬"""
```

#### ì£¼ë¬¸ ë°ì´í„° ìì‚°
```python
@asset(group_name="order")
def src_order(
    context: AssetExecutionContext,
    bigcommerce: BigCommerceResource,
    bigquery: BigQueryResource,
) -> None:
    """BigCommerceì—ì„œ ì£¼ë¬¸ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ BigQueryì— ì ì¬"""
```

#### ìƒí’ˆ ë°ì´í„° ìì‚°
```python
@asset(group_name="product")
def src_product(
    context: AssetExecutionContext,
    bigcommerce: BigCommerceResource,
    bigquery: BigQueryResource,
) -> None:
    """BigCommerceì—ì„œ ìƒí’ˆ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ BigQueryì— ì ì¬"""
```

### 2. Resources (ë¦¬ì†ŒìŠ¤)

#### BigCommerce ë¦¬ì†ŒìŠ¤
```python
class BigCommerceResource(ConfigurableResource):
    access_token: str
    store_hash: str
    
    async def stream_all_customers(self, batch_size: int = 250):
        """ëª¨ë“  ê³ ê° ë°ì´í„°ë¥¼ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°"""
    
    async def stream_all_orders(self, batch_size: int = 50):
        """ëª¨ë“  ì£¼ë¬¸ ë°ì´í„°ë¥¼ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°"""
```

#### PostgreSQL ë¦¬ì†ŒìŠ¤
```python
class PostgresResource(ConfigurableResource):
    postgres_url: str
    
    def execute_query_df(self, query: str) -> pd.DataFrame:
        """SQL ì¿¼ë¦¬ë¥¼ ì‹¤í–‰í•˜ê³  DataFrameìœ¼ë¡œ ë°˜í™˜"""
```

#### BigQuery ë¦¬ì†ŒìŠ¤
```python
# BigQuery ì—°ê²° ì„¤ì •
"bigquery": BigQueryResource(
    project="data-warehouse-455801",
    location="asia-northeast3",
    gcp_credentials=EnvVar("GCP_SERVICE_ACCOUNT_KEY"),
),
```

### 3. DBT í†µí•©

#### DBT Assets
```python
@dbt_assets(
    manifest=transformer_project.manifest_path,
    dagster_dbt_translator=CustomDagsterDbtTranslator(),
)
def transformer_dbt_assets(
    context: AssetExecutionContext, 
    dbt: DbtCliResource, 
    config: DbtConfig
):
    """DBT ëª¨ë¸ì„ ì‹¤í–‰í•˜ëŠ” Asset"""
    dbt_args = ["build"]
    if config.full_refresh:
        dbt_args.append("--full-refresh")
    yield from dbt.cli(dbt_args, context=context).stream()
```

#### DBT ëª¨ë¸ êµ¬ì¡°
- **dim/**: ì°¨ì› í…Œì´ë¸” (ê³ ê°, ìƒí’ˆ, SKU ë“±)
- **fct/**: íŒ©íŠ¸ í…Œì´ë¸” (ì£¼ë¬¸, ë°°ì†¡, í´ë ˆì„ ë“±)
- **mart/**: ë§ˆíŠ¸ í…Œì´ë¸” (ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­)

### 4. Jobs (ì‘ì—…)

#### ê³ ê° ë™ê¸°í™” ì‘ì—…
```python
sync_customer = define_asset_job(
    name="sync_customer",
    selection=AssetSelection.assets(
        src_customer,
        src_customer_metafields,
        ["dim", "dim_customer"],
    ),
)
```

#### ìƒí’ˆ ì•Œë¦¼ ì´ë²¤íŠ¸ ìƒì„±
```python
create_product_notification_event = define_asset_job(
    name="create_product_notification_event",
    selection=AssetSelection.assets(
        src_customer,
        src_customer_metafields,
        ["dim", "dim_customer"],
        bigcommerce_onsale_product_inventory,
        fct_onsale_product_inventory,
        op_product_notification_event,
    ),
)
```

### 5. Schedules (ìŠ¤ì¼€ì¤„)

#### ì¼ì¼ ìŠ¤ì¼€ì¤„
```python
@schedule(
    job=sync_customer,
    cron_schedule="0 2 * * *",  # ë§¤ì¼ ì˜¤ì „ 2ì‹œ
    execution_timezone="Asia/Seoul"
)
def daily_customer_sync(_context):
    return {}
```

#### ë§¤ì‹œê°„ ìŠ¤ì¼€ì¤„
```python
@schedule(
    job=create_product_notification_event,
    cron_schedule="0 * * * *",  # ë§¤ì‹œê°„
    execution_timezone="Asia/Seoul"
)
def hourly_product_notification(_context):
    return {}
```

---

## ğŸ”„ ë°ì´í„° íŒŒì´í”„ë¼ì¸ í”Œë¡œìš°

### 1. ë°ì´í„° ìˆ˜ì§‘ ë‹¨ê³„

```mermaid
graph TD
    A[BigCommerce API] --> B[src_customer]
    A --> C[src_order]
    A --> D[src_product]
    E[PostgreSQL] --> F[src_point_transaction]
    E --> G[src_quotation]
    
    B --> H[BigQuery]
    C --> H
    D --> H
    F --> H
    G --> H
```

### 2. ë°ì´í„° ë³€í™˜ ë‹¨ê³„

```mermaid
graph TD
    A[BigQuery Raw Data] --> B[DBT Models]
    B --> C[dim_customer]
    B --> D[dim_product]
    B --> E[dim_sku]
    B --> F[fct_order]
    B --> G[fct_order_item]
    B --> H[mart_complete_purchase_order]
```

### 3. ì „ì²´ íŒŒì´í”„ë¼ì¸

```mermaid
graph LR
    A[ë°ì´í„° ì†ŒìŠ¤] --> B[Extract Assets]
    B --> C[BigQuery Raw]
    C --> D[DBT Transform]
    D --> E[Data Warehouse]
    
    F[Dagster Orchestration] --> B
    F --> D
    
    G[Monitoring] --> F
    H[Alerts] --> F
    
    I[Schedule] --> F
    J[Sensors] --> F
    
    style A fill:#e1f5fe
    style E fill:#c8e6c9
    style F fill:#fff3e0
    style D fill:#f3e5f5
```

---

## âš¡ ì£¼ìš” ê¸°ëŠ¥

### 1. ë¹„ë™ê¸° ë°ì´í„° ìˆ˜ì§‘
```python
async def src_customer(
    context: AssetExecutionContext,
    config: CustomerConfig,
    bigcommerce: BigCommerceResource,
    bigquery: BigQueryResource,
) -> None:
    """ë¹„ë™ê¸°ë¡œ BigCommerce ê³ ê° ë°ì´í„° ìˆ˜ì§‘"""
    customers = []
    async for customer in bigcommerce.stream_all_customers():
        customers.append(customer)
    
    # BigQueryì— ì ì¬
    df = pd.DataFrame(customers)
    bigquery.load_table_from_dataframe(df, "customers")
```

### 2. ì¦ë¶„ ë°ì´í„° ì²˜ë¦¬
```python
class CustomerConfig(Config):
    hours_lookback: Optional[int] = 12  # 12ì‹œê°„ ì „ë¶€í„°
    full_refresh: bool = False  # ì „ì²´ ìƒˆë¡œê³ ì¹¨ ì—¬ë¶€
```

### 3. ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
```python
@asset_check(asset=products)
def required_columns_has_no_missing_values(context, products):
    """í•„ìˆ˜ ì»¬ëŸ¼ì— ê²°ì¸¡ê°’ì´ ì—†ëŠ”ì§€ ê²€ì‚¬"""
    required_columns = ["id", "name", "price"]
    for col in required_columns:
        if products[col].isnull().any():
            raise CheckError(f"í•„ìˆ˜ ì»¬ëŸ¼ {col}ì— ê²°ì¸¡ê°’ì´ ìˆìŠµë‹ˆë‹¤.")
```

### 4. ì—ëŸ¬ ì²˜ë¦¬ ë° ì•Œë¦¼
```python
@asset
def src_customer(context, bigcommerce, bigquery):
    try:
        # ë°ì´í„° ìˆ˜ì§‘ ë¡œì§
        pass
    except Exception as e:
        context.log.error(f"ê³ ê° ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
        # Slack ì•Œë¦¼ ì „ì†¡
        context.resources.slack.send_message(
            channel="#data-alerts",
            text=f"ê³ ê° ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}"
        )
        raise
```

---

## âš™ï¸ í™˜ê²½ ì„¤ì •

### 1. í™˜ê²½ ë³€ìˆ˜

```bash
# BigCommerce ì„¤ì •
BIGCOMMERCE_ACCESS_TOKEN=your_access_token
BIGCOMMERCE_STORE_HASH=your_store_hash

# PostgreSQL ì„¤ì •
WISE_POSTGRES_URL=postgresql://user:pass@host:port/db
COMMERCE_POSTGRES_URL=postgresql://user:pass@host:port/db

# BigQuery ì„¤ì •
GCP_SERVICE_ACCOUNT_KEY=path/to/service-account.json
LEGACY_GCP_SERVICE_ACCOUNT_KEY=path/to/legacy-service-account.json

# Slack ì„¤ì •
SLACK_BOT_TOKEN=xoxb-your-slack-token
```

### 2. ì˜ì¡´ì„± ì„¤ì¹˜

```bash
pip install -r requirements.txt
```

### 3. DBT ì„¤ì •

```bash
cd transformer
dbt deps
dbt debug
```

---

## ğŸš€ ì‹¤í–‰ ë°©ë²•

### 1. ê°œë°œ í™˜ê²½ ì‹¤í–‰

```bash
# Dagster UI ì‹¤í–‰
dagit -f orchestrator/definitions.py

# íŠ¹ì • Asset ì‹¤í–‰
dagster asset materialize -f orchestrator/definitions.py -s src_customer

# íŠ¹ì • Job ì‹¤í–‰
dagster job execute -f orchestrator/definitions.py -j sync_customer
```

### 2. í”„ë¡œë•ì…˜ ë°°í¬

```bash
# Docker ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t data-governance .

# Docker ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker run -p 3000:3000 data-governance
```

---

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼

### 1. Dagster UI
- âœ… ì‹¤ì‹œê°„ íŒŒì´í”„ë¼ì¸ ìƒíƒœ ëª¨ë‹ˆí„°ë§
- âœ… ì‹¤í–‰ ë¡œê·¸ ë° ë©”íŠ¸ë¦­ í™•ì¸
- âœ… Asset ì˜ì¡´ì„± ê·¸ë˜í”„ ì‹œê°í™”

### 2. Slack ì•Œë¦¼
```python
# ì‹¤íŒ¨ ì‹œ Slack ì•Œë¦¼
context.resources.slack.send_message(
    channel="#data-alerts",
    text=f"Asset {context.asset_key} ì‹¤í–‰ ì‹¤íŒ¨: {error_message}"
)
```

### 3. ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§
```python
@asset_check(asset=src_customer)
def customer_data_quality_check(context, src_customer):
    """ê³ ê° ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬"""
    # ë°ì´í„° ê²€ì¦ ë¡œì§
    pass
```

---

## ğŸ‘¨â€ğŸ’» ê°œë°œ ê°€ì´ë“œ

### 1. ìƒˆë¡œìš´ Asset ì¶”ê°€
```python
@asset(group_name="new_group")
def new_asset(
    context: AssetExecutionContext,
    bigcommerce: BigCommerceResource,
    bigquery: BigQueryResource,
) -> None:
    """ìƒˆë¡œìš´ ë°ì´í„° ìì‚°"""
    # ë°ì´í„° ìˆ˜ì§‘ ë¡œì§
    pass
```

### 2. ìƒˆë¡œìš´ Resource ì¶”ê°€
```python
class NewResource(ConfigurableResource):
    api_key: str
    
    def fetch_data(self):
        """ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        pass
```

### 3. ìƒˆë¡œìš´ Job ì¶”ê°€
```python
new_job = define_asset_job(
    name="new_job",
    selection=AssetSelection.assets(
        "new_asset",
        "dependent_asset",
    ),
)
```

---

## ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. ì¼ë°˜ì ì¸ ë¬¸ì œë“¤

#### BigCommerce API ì œí•œ
```python
# ë°°ì¹˜ í¬ê¸° ì¡°ì •
async def stream_all_customers(self, batch_size: int = 100):
    # ë” ì‘ì€ ë°°ì¹˜ í¬ê¸° ì‚¬ìš©
```

#### BigQuery ì—°ê²° ì˜¤ë¥˜
```python
# ì¸ì¦ íŒŒì¼ ê²½ë¡œ í™•ì¸
"bigquery": BigQueryResource(
    project="data-warehouse-455801",
    gcp_credentials=EnvVar("GCP_SERVICE_ACCOUNT_KEY"),
),
```

#### DBT ëª¨ë¸ ì‹¤í–‰ ì‹¤íŒ¨
```bash
# DBT ë””ë²„ê·¸
cd transformer
dbt debug

# íŠ¹ì • ëª¨ë¸ë§Œ ì‹¤í–‰
dbt run --select model_name
```

### 2. ë¡œê·¸ í™•ì¸
```bash
# Dagster ë¡œê·¸ í™•ì¸
dagster instance logs

# íŠ¹ì • ì‹¤í–‰ ë¡œê·¸ í™•ì¸
dagster run logs <run_id>
```

---

## âš¡ ì„±ëŠ¥ ìµœì í™”

### 1. ë³‘ë ¬ ì²˜ë¦¬
```python
# ë¹„ë™ê¸° ì²˜ë¦¬ë¡œ API í˜¸ì¶œ ìµœì í™”
async def stream_all_customers(self, concurrent_requests: int = 8):
    semaphore = asyncio.Semaphore(concurrent_requests)
    # ë³‘ë ¬ ì²˜ë¦¬ ë¡œì§
```

### 2. ì¦ë¶„ ì²˜ë¦¬
```python
# ì¦ë¶„ ë°ì´í„° ì²˜ë¦¬ë¡œ ì„±ëŠ¥ í–¥ìƒ
class CustomerConfig(Config):
    hours_lookback: Optional[int] = 12  # ìµœê·¼ 12ì‹œê°„ë§Œ ì²˜ë¦¬
```

### 3. ë°°ì¹˜ ì²˜ë¦¬
```python
# ëŒ€ìš©ëŸ‰ ë°ì´í„° ë°°ì¹˜ ì²˜ë¦¬
def process_in_batches(df, batch_size=1000):
    for i in range(0, len(df), batch_size):
        batch = df[i:i+batch_size]
        # ë°°ì¹˜ ì²˜ë¦¬ ë¡œì§
```

---

## ğŸ”’ ë³´ì•ˆ ê³ ë ¤ì‚¬í•­

### 1. í™˜ê²½ ë³€ìˆ˜ ê´€ë¦¬
- âœ… ë¯¼ê°í•œ ì •ë³´ëŠ” í™˜ê²½ ë³€ìˆ˜ë¡œ ê´€ë¦¬
- âœ… í”„ë¡œë•ì…˜ì—ì„œëŠ” ì‹œí¬ë¦¿ ë§¤ë‹ˆì € ì‚¬ìš©

### 2. ë°ì´í„° ì ‘ê·¼ ì œì–´
```python
# PostgreSQL ë¦¬ì†ŒìŠ¤ì—ì„œ INSERT/DELETE ì°¨ë‹¨
def execute_query_df(self, query: str) -> pd.DataFrame:
    query_upper = query.upper()
    if "INSERT" in query_upper or "DELETE" in query_upper:
        raise ValueError("ë³´ì•ˆìƒì˜ ì´ìœ ë¡œ INSERTì™€ DELETE ì¿¼ë¦¬ëŠ” ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
```

### 3. API í‚¤ ê´€ë¦¬
- âœ… BigCommerce API í‚¤ëŠ” í™˜ê²½ ë³€ìˆ˜ë¡œ ê´€ë¦¬
- âœ… ì •ê¸°ì ì¸ í‚¤ ë¡œí…Œì´ì…˜

---

## ğŸ“š ê²°ë¡ 

ì´ ë°ì´í„° ê±°ë²„ë„ŒìŠ¤ í”„ë¡œì íŠ¸ëŠ” Dagsterë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ë¥¼ í†µí•©í•˜ê³ , DBTë¥¼ í†µí•´ ë°ì´í„°ë¥¼ ë³€í™˜í•˜ì—¬ ì¼ê´€ëœ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.

### ì£¼ìš” íŠ¹ì§•
- **í™•ì¥ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜**: ìƒˆë¡œìš´ ë°ì´í„° ì†ŒìŠ¤ì™€ ë³€í™˜ ë¡œì§ì„ ì‰½ê²Œ ì¶”ê°€
- **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**: Dagster UIë¥¼ í†µí•œ ì‹¤ì‹œê°„ íŒŒì´í”„ë¼ì¸ ëª¨ë‹ˆí„°ë§
- **ë°ì´í„° í’ˆì§ˆ ë³´ì¥**: Asset ì²´í¬ë¥¼ í†µí•œ ë°ì´í„° í’ˆì§ˆ ê²€ì¦
- **ìë™í™”ëœ ìŠ¤ì¼€ì¤„ë§**: ì •ê¸°ì ì¸ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
- **ì—ëŸ¬ ì²˜ë¦¬ ë° ì•Œë¦¼**: ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ ì•Œë¦¼ìœ¼ë¡œ ë¹ ë¥¸ ëŒ€ì‘

---

## ğŸ“– ì°¸ê³  ìë£Œ

- **Dagster ê³µì‹ ë¬¸ì„œ**: [https://docs.dagster.io/](https://docs.dagster.io/)
- **DBT ê³µì‹ ë¬¸ì„œ**: [https://docs.getdbt.com/](https://docs.getdbt.com/)
- **BigCommerce API ë¬¸ì„œ**: [https://developer.bigcommerce.com/](https://developer.bigcommerce.com/)
- **BigQuery ë¬¸ì„œ**: [https://cloud.google.com/bigquery/docs](https://cloud.google.com/bigquery/docs)

---

<div align="center">

**Made with â¤ï¸ for Wisely Data Team**

</div> 